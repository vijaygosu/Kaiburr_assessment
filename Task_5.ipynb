{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3853839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading and Preparing Data ---\n",
      "Dataset loaded successfully with 11522175 rows.\n",
      "Filtered to 3521497 rows with target products.\n",
      "Kept 1323203 rows with non-empty complaint narratives.\n",
      "\n",
      "Class Distribution:\n",
      "Product\n",
      "Credit reporting, credit repair services, or other personal consumer reports    807276\n",
      "Debt collection                                                                 371629\n",
      "Mortgage                                                                        134837\n",
      "Consumer Loan                                                                     9461\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- Step 2: Text Pre-Processing ---\n",
      "Applying preprocessing to complaint narratives...\n",
      "Preprocessing complete.\n",
      "\n",
      "\n",
      "Data split into 1058562 training samples and 264641 testing samples.\n",
      "\n",
      "\n",
      "--- Steps 4 & 5: Training, Comparing, and Evaluating Models ---\n",
      "--- Training and Evaluating Multinomial Naive Bayes ---\n",
      "Classification Report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Credit reporting, repair, or other       0.92      0.88      0.90    161456\n",
      "                   Debt collection       0.81      0.80      0.80     74326\n",
      "                     Consumer Loan       0.36      0.24      0.29      1892\n",
      "                          Mortgage       0.75      0.96      0.84     26967\n",
      "\n",
      "                          accuracy                           0.86    264641\n",
      "                         macro avg       0.71      0.72      0.71    264641\n",
      "                      weighted avg       0.87      0.86      0.86    264641\n",
      "\n",
      "Confusion Matrix:\n",
      "[[142859  13128    596   4873]\n",
      " [ 11765  59112    225   3224]\n",
      " [   281    416    463    732]\n",
      " [   616    536      5  25810]]\n",
      "\n",
      "\n",
      "--- Training and Evaluating Logistic Regression ---\n",
      "Classification Report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Credit reporting, repair, or other       0.92      0.94      0.93    161456\n",
      "                   Debt collection       0.88      0.84      0.86     74326\n",
      "                     Consumer Loan       0.69      0.35      0.46      1892\n",
      "                          Mortgage       0.92      0.93      0.92     26967\n",
      "\n",
      "                          accuracy                           0.91    264641\n",
      "                         macro avg       0.85      0.76      0.79    264641\n",
      "                      weighted avg       0.91      0.91      0.91    264641\n",
      "\n",
      "Confusion Matrix:\n",
      "[[152509   7678    138   1131]\n",
      " [ 11137  62182    149    858]\n",
      " [   547    500    662    183]\n",
      " [  1268    684     15  25000]]\n",
      "\n",
      "\n",
      "--- Training and Evaluating Linear SVM ---\n",
      "Classification Report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Credit reporting, repair, or other       0.92      0.95      0.93    161456\n",
      "                   Debt collection       0.88      0.84      0.86     74326\n",
      "                     Consumer Loan       0.79      0.28      0.42      1892\n",
      "                          Mortgage       0.92      0.93      0.92     26967\n",
      "\n",
      "                          accuracy                           0.91    264641\n",
      "                         macro avg       0.88      0.75      0.78    264641\n",
      "                      weighted avg       0.91      0.91      0.91    264641\n",
      "\n",
      "Confusion Matrix:\n",
      "[[152851   7369     60   1176]\n",
      " [ 11237  62112     73    904]\n",
      " [   591    559    535    207]\n",
      " [  1238    668      6  25055]]\n",
      "\n",
      "\n",
      "Best performing model is: Linear SVM with a weighted F1-score of 0.9073\n",
      "\n",
      "\n",
      "--- Step 6: Prediction on a New Complaint ---\n",
      "New Complaint Text:\n",
      "'I am writing to dispute a charge on my mortgage account.\n",
      "My bank, Acme Bank, has incorrectly charged me a late fee,\n",
      "but I sent the payment well before the due date. I have bank\n",
      "records to prove it. This has negatively affected my credit score.'\n",
      "------------------------------\n",
      "Predicted Category ID: 3\n",
      "Predicted Product: 'Mortgage'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Download NLTK data (only needs to be done once) ---\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    # This is for the lemmatizer\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK wordnet...\")\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# --- 1. Explanatory Data Analysis and Feature Engineering ---\n",
    "print(\"--- Step 1: Loading and Preparing Data ---\")\n",
    "\n",
    "# Load the dataset from the provided URL\n",
    "DATA_PATH = \"complaints.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, on_bad_lines='skip')\n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the target categories as specified in the task\n",
    "TARGET_PRODUCTS = {\n",
    "    \"Credit reporting, repair, or other\": \"Credit reporting, credit repair services, or other personal consumer reports\",\n",
    "    \"Debt collection\": \"Debt collection\",\n",
    "    \"Consumer Loan\": \"Consumer Loan\",\n",
    "    \"Mortgage\": \"Mortgage\"\n",
    "}\n",
    "\n",
    "# Filter the dataframe to only include the products we are interested in\n",
    "df_filtered = df[df['Product'].isin(TARGET_PRODUCTS.values())].copy()\n",
    "print(f\"Filtered to {len(df_filtered)} rows with target products.\")\n",
    "\n",
    "# Drop rows where the complaint narrative is missing, as it's our feature\n",
    "df_filtered.dropna(subset=['Consumer complaint narrative'], inplace=True)\n",
    "print(f\"Kept {len(df_filtered)} rows with non-empty complaint narratives.\")\n",
    "\n",
    "\n",
    "# Create a mapping from product names to numerical labels (0, 1, 2, 3)\n",
    "product_to_label = {\n",
    "    \"Credit reporting, credit repair services, or other personal consumer reports\": 0,\n",
    "    \"Debt collection\": 1,\n",
    "    \"Consumer Loan\": 2,\n",
    "    \"Mortgage\": 3\n",
    "}\n",
    "df_filtered['label'] = df_filtered['Product'].map(product_to_label)\n",
    "label_to_product = {v: k for k, v in product_to_label.items()}\n",
    "\n",
    "# Display the class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df_filtered['Product'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Text Pre-Processing ---\n",
    "print(\"--- Step 2: Text Pre-Processing ---\")\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and prepares text data for modeling.\"\"\"\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # 3. Tokenize\n",
    "    words = text.split()\n",
    "    # 4. Remove stopwords and lemmatize\n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply the preprocessing function to the complaint narratives\n",
    "# This might take a minute or two depending on the dataset size\n",
    "print(\"Applying preprocessing to complaint narratives...\")\n",
    "df_filtered['cleaned_narrative'] = df_filtered['Consumer complaint narrative'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_filtered['cleaned_narrative']\n",
    "y = df_filtered['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Data split into {len(X_train)} training samples and {len(X_test)} testing samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 3. Selection of Multi Classification Models ---\n",
    "# We will compare three popular models for text classification:\n",
    "# - Multinomial Naive Bayes\n",
    "# - Logistic Regression\n",
    "# - Linear Support Vector Machine (SVM)\n",
    "\n",
    "models = {\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Linear SVM\": LinearSVC(random_state=42)\n",
    "}\n",
    "\n",
    "# --- 4 & 5. Comparison of Model Performance & Evaluation ---\n",
    "print(\"--- Steps 4 & 5: Training, Comparing, and Evaluating Models ---\")\n",
    "\n",
    "best_model = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"--- Training and Evaluating {model_name} ---\")\n",
    "\n",
    "    # Create a pipeline that first vectorizes the text and then applies the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', model_instance)\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    report = classification_report(y_test, y_pred, target_names=TARGET_PRODUCTS.keys(), output_dict=True)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=TARGET_PRODUCTS.keys()))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Check if this model is the best one so far based on weighted F1-score\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    if weighted_f1 > best_f1_score:\n",
    "        best_f1_score = weighted_f1\n",
    "        best_model = pipeline\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(f\"Best performing model is: {best_model_name} with a weighted F1-score of {best_f1_score:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 6. Prediction ---\n",
    "print(\"--- Step 6: Prediction on a New Complaint ---\")\n",
    "\n",
    "# A new, unseen complaint\n",
    "new_complaint = \"\"\"\n",
    "I am writing to dispute a charge on my mortgage account.\n",
    "My bank, Acme Bank, has incorrectly charged me a late fee,\n",
    "but I sent the payment well before the due date. I have bank\n",
    "records to prove it. This has negatively affected my credit score.\n",
    "\"\"\"\n",
    "\n",
    "# Use the best model to predict the category\n",
    "predicted_label = best_model.predict([new_complaint])[0]\n",
    "predicted_product = label_to_product[predicted_label]\n",
    "\n",
    "print(f\"New Complaint Text:\\n'{new_complaint.strip()}'\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Predicted Category ID: {predicted_label}\")\n",
    "print(f\"Predicted Product: '{predicted_product}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a74b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading and Preparing Data ---\n",
      "Dataset loaded successfully with 11522175 rows.\n",
      "Filtered to 3521497 rows with target products.\n",
      "Kept 1323203 rows with non-empty complaint narratives.\n",
      "\n",
      "Class Distribution:\n",
      "Product\n",
      "Credit reporting, credit repair services, or other personal consumer reports    807276\n",
      "Debt collection                                                                 371629\n",
      "Mortgage                                                                        134837\n",
      "Consumer Loan                                                                     9461\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- Step 2: Text Pre-Processing ---\n",
      "Applying preprocessing to complaint narratives...\n",
      "Preprocessing complete.\n",
      "\n",
      "\n",
      "Data split into 1058562 training samples and 264641 testing samples.\n",
      "\n",
      "\n",
      "--- Steps 4 & 5: Training, Comparing, and Evaluating Models ---\n",
      "--- Training and Evaluating Multinomial Naive Bayes ---\n",
      "Classification Report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Credit reporting, repair, or other       0.92      0.88      0.90    161456\n",
      "                   Debt collection       0.81      0.80      0.80     74326\n",
      "                     Consumer Loan       0.36      0.24      0.29      1892\n",
      "                          Mortgage       0.75      0.96      0.84     26967\n",
      "\n",
      "                          accuracy                           0.86    264641\n",
      "                         macro avg       0.71      0.72      0.71    264641\n",
      "                      weighted avg       0.87      0.86      0.86    264641\n",
      "\n",
      "Confusion Matrix:\n",
      "[[142859  13128    596   4873]\n",
      " [ 11765  59112    225   3224]\n",
      " [   281    416    463    732]\n",
      " [   616    536      5  25810]]\n",
      "\n",
      "\n",
      "--- Training and Evaluating Linear SVM ---\n",
      "Classification Report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Credit reporting, repair, or other       0.92      0.95      0.93    161456\n",
      "                   Debt collection       0.88      0.84      0.86     74326\n",
      "                     Consumer Loan       0.79      0.28      0.42      1892\n",
      "                          Mortgage       0.92      0.93      0.92     26967\n",
      "\n",
      "                          accuracy                           0.91    264641\n",
      "                         macro avg       0.88      0.75      0.78    264641\n",
      "                      weighted avg       0.91      0.91      0.91    264641\n",
      "\n",
      "Confusion Matrix:\n",
      "[[152851   7369     60   1176]\n",
      " [ 11237  62112     73    904]\n",
      " [   591    559    535    207]\n",
      " [  1238    668      6  25055]]\n",
      "\n",
      "\n",
      "--- Training and Evaluating Random Forest ---\n"
     ]
    }
   ],
   "source": [
    "# --- Import necessary libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Download NLTK data (only needs to be done once) ---\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    # This is for the lemmatizer\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK wordnet...\")\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# --- 1. Explanatory Data Analysis and Feature Engineering ---\n",
    "print(\"--- Step 1: Loading and Preparing Data ---\")\n",
    "\n",
    "# Load the dataset from the provided URL\n",
    "DATA_PATH = \"complaints.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, on_bad_lines='skip')\n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the target categories as specified in the task\n",
    "TARGET_PRODUCTS = {\n",
    "    \"Credit reporting, repair, or other\": \"Credit reporting, credit repair services, or other personal consumer reports\",\n",
    "    \"Debt collection\": \"Debt collection\",\n",
    "    \"Consumer Loan\": \"Consumer Loan\",\n",
    "    \"Mortgage\": \"Mortgage\"\n",
    "}\n",
    "\n",
    "# Filter the dataframe to only include the products we are interested in\n",
    "df_filtered = df[df['Product'].isin(TARGET_PRODUCTS.values())].copy()\n",
    "print(f\"Filtered to {len(df_filtered)} rows with target products.\")\n",
    "\n",
    "# Drop rows where the complaint narrative is missing, as it's our feature\n",
    "df_filtered.dropna(subset=['Consumer complaint narrative'], inplace=True)\n",
    "print(f\"Kept {len(df_filtered)} rows with non-empty complaint narratives.\")\n",
    "\n",
    "\n",
    "# Create a mapping from product names to numerical labels (0, 1, 2, 3)\n",
    "product_to_label = {\n",
    "    \"Credit reporting, credit repair services, or other personal consumer reports\": 0,\n",
    "    \"Debt collection\": 1,\n",
    "    \"Consumer Loan\": 2,\n",
    "    \"Mortgage\": 3\n",
    "}\n",
    "df_filtered['label'] = df_filtered['Product'].map(product_to_label)\n",
    "label_to_product = {v: k for k, v in product_to_label.items()}\n",
    "\n",
    "# Display the class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df_filtered['Product'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Text Pre-Processing ---\n",
    "print(\"--- Step 2: Text Pre-Processing ---\")\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and prepares text data for modeling.\"\"\"\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # 3. Tokenize\n",
    "    words = text.split()\n",
    "    # 4. Remove stopwords and lemmatize\n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply the preprocessing function to the complaint narratives\n",
    "# This might take a minute or two depending on the dataset size\n",
    "print(\"Applying preprocessing to complaint narratives...\")\n",
    "df_filtered['cleaned_narrative'] = df_filtered['Consumer complaint narrative'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_filtered['cleaned_narrative']\n",
    "y = df_filtered['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Data split into {len(X_train)} training samples and {len(X_test)} testing samples.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 3. Selection of Multi Classification Models ---\n",
    "# We will compare three popular models for text classification:\n",
    "# - Multinomial Naive Bayes\n",
    "# - Linear Support Vector Machine (SVM)\n",
    "# - Random Forest\n",
    "models = {\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Linear SVM\": LinearSVC(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# --- 4 & 5. Comparison of Model Performance & Evaluation ---\n",
    "print(\"--- Steps 4 & 5: Training, Comparing, and Evaluating Models ---\")\n",
    "\n",
    "best_model = None\n",
    "best_f1_score = 0.0\n",
    "best_model_name = \"\"\n",
    "\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"--- Training and Evaluating {model_name} ---\")\n",
    "\n",
    "    # Create a pipeline that first vectorizes the text and then applies the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', model_instance)\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    report = classification_report(y_test, y_pred, target_names=TARGET_PRODUCTS.keys(), output_dict=True)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=TARGET_PRODUCTS.keys()))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Check if this model is the best one so far based on weighted F1-score\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    if weighted_f1 > best_f1_score:\n",
    "        best_f1_score = weighted_f1\n",
    "        best_model = pipeline\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(f\"Best performing model is: {best_model_name} with a weighted F1-score of {best_f1_score:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 6. Prediction ---\n",
    "print(\"--- Step 6: Prediction on a New Complaint ---\")\n",
    "\n",
    "# A new, unseen complaint\n",
    "new_complaint = \"\"\"\n",
    "I am writing to dispute a charge on my mortgage account.\n",
    "My bank, Acme Bank, has incorrectly charged me a late fee,\n",
    "but I sent the payment well before the due date. I have bank\n",
    "records to prove it. This has negatively affected my credit score.\n",
    "\"\"\"\n",
    "\n",
    "# Use the best model to predict the category\n",
    "predicted_label = best_model.predict([new_complaint])[0]\n",
    "predicted_product = label_to_product[predicted_label]\n",
    "\n",
    "print(f\"New Complaint Text:\\n'{new_complaint.strip()}'\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Predicted Category ID: {predicted_label}\")\n",
    "print(f\"Predicted Product: '{predicted_product}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd95f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vijay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Step 1: Loading and Preparing Data ---\n",
      "Dataset loaded successfully with 11522175 rows from 'complaints.csv'.\n",
      "Filtered to 1323203 rows with target products and narratives.\n",
      "Using a smaller sample of 4000 rows for faster training.\n",
      "\n",
      "--- Step 2: Setting up BERT Tokenizer and PyTorch Datasets ---\n",
      "DataLoaders created.\n",
      "\n",
      "--- Step 3: Loading Pre-trained BERT Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model loaded and moved to device.\n",
      "\n",
      "--- Step 4: Fine-Tuning the Model ---\n",
      "======== Epoch 1 / 2 ========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 173\u001b[0m\n\u001b[0;32m    169\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    171\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 173\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    180\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1675\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1675\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1687\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1689\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1144\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1156\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1157\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pytorch_utils.py:253\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Kaiburr Assessment - Task 5: Data Science Example with BERT\n",
    "# Name: Vijay\n",
    "# Date: 2024-10-18\n",
    "\n",
    "# This script performs text classification using a pre-trained BERT model.\n",
    "# It leverages the Hugging Face transformers and PyTorch libraries.\n",
    "# NOTE: To run this, you need to install torch and transformers:\n",
    "# pip install torch transformers pandas scikit-learn\n",
    "\n",
    "# --- Import necessary libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# --- CORRECTED IMPORT ---\n",
    "# AdamW is now imported from torch.optim, not transformers\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 256  # Max length of the tokenized text\n",
    "BATCH_SIZE = 8 # Batch size for training (reduce if you have memory issues)\n",
    "EPOCHS = 2     # Number of training epochs\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# --- Check for GPU availability ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "\n",
    "# --- 1. Explanatory Data Analysis and Feature Engineering ---\n",
    "print(\"--- Step 1: Loading and Preparing Data ---\")\n",
    "\n",
    "# IMPORTANT: Since you have downloaded the dataset, replace 'complaints.csv'\n",
    "# with the exact path to your unzipped CSV file if it's different or in another folder.\n",
    "LOCAL_DATA_PATH = \"complaints.csv\"\n",
    "try:\n",
    "    # Specifying dtype can help with memory allocation issues on loading large CSVs\n",
    "    df = pd.read_csv(LOCAL_DATA_PATH, on_bad_lines='skip', dtype={'Consumer complaint narrative': 'string', 'Product': 'string'})\n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows from '{LOCAL_DATA_PATH}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at '{LOCAL_DATA_PATH}'.\")\n",
    "    print(\"Please make sure this script is in the same directory as your CSV file, or provide the full file path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the target categories\n",
    "TARGET_PRODUCTS = {\n",
    "    \"Credit reporting, credit repair services, or other personal consumer reports\": 0,\n",
    "    \"Debt collection\": 1,\n",
    "    \"Consumer Loan\": 2,\n",
    "    \"Mortgage\": 3\n",
    "}\n",
    "LABEL_NAMES = list(TARGET_PRODUCTS.keys())\n",
    "\n",
    "# Filter the dataframe\n",
    "df_filtered = df[df['Product'].isin(TARGET_PRODUCTS.keys())].copy()\n",
    "df_filtered.dropna(subset=['Consumer complaint narrative'], inplace=True)\n",
    "\n",
    "# Map product names to numerical labels\n",
    "df_filtered['label'] = df_filtered['Product'].map(TARGET_PRODUCTS)\n",
    "print(f\"Filtered to {len(df_filtered)} rows with target products and narratives.\")\n",
    "\n",
    "# --- DATA SAMPLING FOR QUICK DEMONSTRATION ---\n",
    "# Fine-tuning BERT on the full dataset can take a long time on a CPU.\n",
    "# We will sample the data to make the script run faster for demonstration.\n",
    "# For the final submission, you might want to use a larger sample or the full dataset.\n",
    "df_sample = df_filtered.sample(n=4000, random_state=42)\n",
    "print(f\"Using a smaller sample of {len(df_sample)} rows for faster training.\\n\")\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_sample['Consumer complaint narrative'],\n",
    "    df_sample['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_sample['label']\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2. Text Pre-Processing (BERT Tokenization) & Dataset Creation ---\n",
    "print(\"--- Step 2: Setting up BERT Tokenizer and PyTorch Datasets ---\")\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class ComplaintDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts.iloc[item])\n",
    "        label = self.labels.iloc[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_dataset = ComplaintDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "val_dataset = ComplaintDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"DataLoaders created.\\n\")\n",
    "\n",
    "# --- 3. Selection of Model (BERT) ---\n",
    "print(\"--- Step 3: Loading Pre-trained BERT Model ---\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(TARGET_PRODUCTS)\n",
    ")\n",
    "model.to(device)\n",
    "print(\"BERT model loaded and moved to device.\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Model Fine-Tuning (Training) ---\n",
    "print(\"--- Step 4: Fine-Tuning the Model ---\")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "# --- CORRECTED OPTIMIZER INITIALIZATION ---\n",
    "# The 'correct_bias' argument is removed as it's not used by torch.optim.AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'======== Epoch {epoch + 1} / {EPOCHS} ========')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'  Average training loss: {avg_train_loss:.2f}')\n",
    "\n",
    "print(\"\\nTraining complete.\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Model Evaluation ---\n",
    "print(\"--- Step 5: Evaluating the Model ---\")\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Map integer labels back to product names for the report\n",
    "target_names_for_report = [name.split(',')[0] for name in TARGET_PRODUCTS.keys()] # Shorten names for readability\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=target_names_for_report))\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 6. Prediction ---\n",
    "print(\"--- Step 6: Prediction on a New Complaint ---\")\n",
    "\n",
    "# A new, unseen complaint\n",
    "new_complaint = \"\"\"\n",
    "I am writing to dispute a charge on my mortgage account.\n",
    "My bank, Acme Bank, has incorrectly charged me a late fee,\n",
    "but I sent the payment well before the due date. I have bank\n",
    "records to prove it. This has negatively affected my credit score.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the new complaint\n",
    "encoded_complaint = tokenizer.encode_plus(\n",
    "    new_complaint,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "# Make prediction\n",
    "input_ids = encoded_complaint['input_ids'].to(device)\n",
    "attention_mask = encoded_complaint['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "# Map label index to product name\n",
    "predicted_product = LABEL_NAMES[prediction]\n",
    "\n",
    "print(f\"New Complaint Text:\\n'{new_complaint.strip()}'\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Predicted Category ID: {prediction}\")\n",
    "print(f\"Predicted Product: '{predicted_product}'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
